{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51034f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saadi\\anaconda3\\envs\\DL_Pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8158caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the positional encoding function \n",
    "class PositionalEncoding(nn.Module):\n",
    "    ''' This function helps to preserve the order of the words in a sequence by encoding the position of each word and add \n",
    "    it to its corresponding embedding'''\n",
    "    def __init__(self, embed_size, max_length,device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "        self.pe_matrix = torch.zeros(self.max_length,self.embed_size).to(device)\n",
    "\n",
    "    def forward(self, embedding):\n",
    "        for pos in range(self.max_length):\n",
    "            for i in range(self.embed_size,2):\n",
    "                a = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                self.pe_matrix[pos,i] = math.sin(pos / (10000 ** ((2 * i)/self.embed_dim)))\n",
    "                self.pe_matrix[pos,i+1] =  math.cos(pos / (10000 ** ((2 * (i + 1))/self.embed_dim)))\n",
    "        # Reshape the pe_matrix where position 0 is of dimension 1\n",
    "        self.pe_matrix = self.pe_matrix.unsqueeze(0)\n",
    "        seq_length = embedding.size(1)\n",
    "        # add the positional encoding matrix to the embedding matrix\n",
    "        output = embedding + self.pe_matrix[:, :seq_length]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be010a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code the multihead attention layer\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        assert self.embed_dim == self.head_dim * self.num_heads\n",
    "\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.dense_out = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "    def forward(self, v,k,q, mask=None):\n",
    "        # v,q, and k are equal to the output of the embedding layer\n",
    "        v = self.value(v)\n",
    "        k = self.key(k)\n",
    "        q = self.query(q)\n",
    "        # N is the number of samples \n",
    "        N = q.size(0)\n",
    "        # query_length is the embed size of each query.\n",
    "        query_length = q.size(1)\n",
    "        # split the v,k, and q into the number of heads\n",
    "        v = v.reshape(v.size(0), v.size(1), self.num_heads, self.head_dim)\n",
    "        k = k.reshape(k.size(0), k.size(1), self.num_heads, self.head_dim)\n",
    "        q = q.reshape(q.size(0), q.size(1), self.num_heads, self.head_dim)\n",
    "        # multiply queries and keys with einsum\n",
    "        attention_weights = torch.einsum(\"nqhd,nkhd ->nhqk\",[q,k])\n",
    "        if mask is not None:\n",
    "            attention_weights =  attention_weights.masked_fill(mask==0, float(\"-1e20\"))\n",
    "       \n",
    "        # apply a softmax on the attention_weights to get probabilities as output \n",
    "        attention_weights = nn.Softmax(dim=3)(attention_weights/math.sqrt(self.head_dim))\n",
    "        # multiply the attention_weights by the values v\n",
    "        attention = torch.einsum(\"nhqk,nkhd ->nqhd\",[attention_weights,v]) # k and v always have the same size (length)\n",
    "        # reshape the output attention matrix to the initial input shape\n",
    "        attention = attention.reshape(N,query_length, self.num_heads * self.head_dim)\n",
    "        # add the linear layer \n",
    "        output = self.dense_out(attention)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791efe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the whole encoder block  which is composed of the embedding layer, multiheadattention layer, normalization layer,\n",
    "# and linear layer\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, emed_size, num_heads):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(emed_size,num_heads)\n",
    "        self.norm1 = nn.LayerNorm(emed_size)\n",
    "        self.norm2 = nn.LayerNorm(emed_size)\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(emed_size,emed_size),\n",
    "                                          nn.ReLU(),\n",
    "                                          nn.Linear(emed_size,emed_size))\n",
    "    def forward(self, v,k,q,mask):\n",
    "        attention = self.attention(v,k,q,mask)\n",
    "        x = self.norm1(attention + q)\n",
    "        forward = self.feed_forward(x)\n",
    "        output = self.norm2(x + forward)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0326047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the encoder which composes of a stack of encoder blocks\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,embed_size, num_heads, vocab_size,max_length, num_layers, device):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(self.vocab_size,self.embed_size).to(self.device)\n",
    "        self.positional_encoding = PositionalEncoding(self.embed_size, self.max_length, self.device)\n",
    "        # ModuleList is a list of encoder blocks\n",
    "        self.layers = nn.ModuleList([EncoderBlock(self.embed_size, self.num_heads) for i in range(self.num_layers)]).to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, input, mask):\n",
    "\n",
    "        x = self.embedding(input)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,x,x,mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "653a531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder block which is quite similar to the encoder block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.transformer = EncoderBlock(self.embed_size, self.num_heads)\n",
    "        self.attention = MultiHeadSelfAttention(self.embed_size,self.num_heads)\n",
    "        self.norm = nn.LayerNorm(self.embed_size)\n",
    "\n",
    "    def forward(self,input, value, key, encoder_mask, decoder_mask):\n",
    "\n",
    "        x = self.attention(input, input, input, decoder_mask)\n",
    "        out1 = self.norm(input+x)\n",
    "        out2 = self.transformer(value, key, out1, encoder_mask)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19f75207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decoder block which composes of a stack of decoder blocks\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,embed_size, num_heads, decoder_vocab_size, max_length,num_layers,device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.decoder_vocab_size = decoder_vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(self.decoder_vocab_size, self.embed_size).to(self.device)\n",
    "        self.positional_encoding = PositionalEncoding(self.embed_size, self.max_length, self.device)\n",
    "        self.layers = nn.ModuleList([DecoderBlock(self.embed_size, self.num_heads) for i in range(self.num_layers)]).to(self.device)\n",
    "        self.output = nn.Linear(embed_size,decoder_vocab_size).to(self.device)\n",
    "\n",
    "    def forward(self, input, encoder_out, encoder_mask, decoder_mask):\n",
    "        x = self.embedding(input)\n",
    "        x = self.positional_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_out,encoder_out,encoder_mask,decoder_mask)\n",
    "\n",
    "        output = self.output(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee0b50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the transformer block from the encoder and decoder\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,embed_size, num_heads, encoder_vocab_size,decoder_vocab_size, max_length, num_layers, encoder_pad_idx, decoder_pad_idx,device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder_pad_idx = encoder_pad_idx\n",
    "        self.decoder_pad_idx = decoder_pad_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(embed_size, num_heads, encoder_vocab_size,max_length, num_layers,self.device)\n",
    "        self.decoder = Decoder(embed_size, num_heads, decoder_vocab_size,max_length, num_layers,self.device)\n",
    "\n",
    "\n",
    "    def make_encoder_mask(self,encoder_input):\n",
    "\n",
    "        encoder_mask = (encoder_input != self.encoder_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return encoder_mask.to(self.device)\n",
    "\n",
    "\n",
    "    def make_decoder_mask(self, decoder_input):\n",
    "\n",
    "        N, decoder_input_len = decoder_input.shape\n",
    "        # torch.tril set the upper part of a tensor to zero\n",
    "        # torch.expand expand the tensor by replicating rows and columns\n",
    "        decoder_mask = torch.tril(torch.ones((decoder_input_len,decoder_input_len))).expand(N,1,decoder_input_len,decoder_input_len)\n",
    "        return decoder_mask .to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, encoder_input, decoder_input):\n",
    "\n",
    "        encoder_mask = self.make_encoder_mask(encoder_input)\n",
    "        decoder_mask = self.make_decoder_mask(decoder_input)\n",
    "        enc_output = self.encoder(encoder_input, encoder_mask)\n",
    "        final_output = self.decoder(decoder_input, enc_output, encoder_mask, decoder_mask)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "009f9a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(10, 64)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (2): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (3): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (4): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (5): EncoderBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (feed_forward): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(10, 64)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (layers): ModuleList(\n",
      "      (0): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): DecoderBlock(\n",
      "        (transformer): EncoderBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Sequential(\n",
      "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "          (dense_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 8, 10])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x = torch.tensor(([1,5,6,4,3,9,5,2,0],[1,5,7,3,4,5,6,7,2])).to(device)\n",
    "    trg = torch.tensor(([1,7,4,3,5,9,2,0],[1,5,6,2,4,7,6,2])).to(device)\n",
    "    encoder_pad_idx = 0\n",
    "    decoder_pad_idx = 0\n",
    "    encoder_vocab_size = 10\n",
    "    decoder_vocab_size = 10\n",
    "    max_length = 9\n",
    "    num_layers = 6\n",
    "    embed_size = 64\n",
    "    num_heads = 8\n",
    "\n",
    "    model = Transformer(embed_size, num_heads,encoder_vocab_size,decoder_vocab_size, max_length, num_layers,encoder_pad_idx, decoder_pad_idx,device)\n",
    "    print(model)\n",
    "    a = trg[:,:-1]\n",
    "    out= model(x, trg)\n",
    "    print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
